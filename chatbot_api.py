from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from typing import List, Dict, Optional
import os
import re
import json
from rank_bm25 import BM25Okapi
from google import genai
from dotenv import load_dotenv

load_dotenv()

# ============================================================================
# KONFIGÃœRASYON
# ============================================================================

app = FastAPI(title="Mevzuat Chatbot API")

CHROMA_DIR = "./mevzuat_db"
JSON_PATH = "tum_mevzuat_maddeleri_enriched.json"

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")

# ============================================================================
# MODEL YÃœKLEMELERÄ°
# ============================================================================

print("ğŸ¤– BERTurk embedding modeli yÃ¼kleniyor...")
embeddings = HuggingFaceEmbeddings(
    model_name="dbmdz/bert-base-turkish-cased",
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)
print("âœ… BERTurk hazÄ±r!")

if not os.path.exists(CHROMA_DIR):
    raise Exception(f"âŒ ChromaDB bulunamadÄ±: {CHROMA_DIR}")

print("ğŸ“š ChromaDB yÃ¼kleniyor...")
db = Chroma(persist_directory=CHROMA_DIR, embedding_function=embeddings)
print("âœ… ChromaDB hazÄ±r!")

if not os.path.exists(JSON_PATH):
    raise Exception(f"âŒ JSON dosyasÄ± bulunamadÄ±: {JSON_PATH}")

print("ğŸ“„ Mevzuat dÃ¶kÃ¼manlarÄ± yÃ¼kleniyor...")
with open(JSON_PATH, "r", encoding="utf-8") as f:
    all_documents = json.load(f)
print(f"âœ… {len(all_documents)} madde yÃ¼klendi")

print("ğŸ“Š BM25 index oluÅŸturuluyor...")
tokenized_corpus = [doc["icerik"].lower().split() for doc in all_documents]
bm25 = BM25Okapi(tokenized_corpus)
print("âœ… BM25 hazÄ±r!")

print("ğŸ§  Gemini LLM yapÄ±landÄ±rÄ±lÄ±yor...")
try:
    client = genai.Client(api_key=GEMINI_API_KEY)
    MODEL_NAME = 'gemini-2.5-flash'
    print("âœ… Gemini LLM hazÄ±r!")
    gemini_available = True
except Exception as e:
    print(f"âŒ Gemini yapÄ±landÄ±rÄ±lamadÄ±: {e}")
    gemini_available = False


# ============================================================================
# PYDANTIC MODELLERÄ°
# ============================================================================

class Question(BaseModel):
    question: str
    fakulte_filter: Optional[str] = None  # ğŸ†• YENÄ° PARAMETRE
    session_id: str = "default"
    top_k: int = 5
    temperature: float = 0.3


class ChatResponse(BaseModel):
    question: str
    answer: str
    sources: list
    session_id: str


# ============================================================================
# ğŸ†• FAKÃœLTE TESPÄ°T FONKSÄ°YONU
# ============================================================================

def detect_fakulte(query: str) -> Optional[str]:
    """Sorgudan fakÃ¼lte/birim tespit et"""
    query_lower = query.lower()

    # FakÃ¼lte anahtar kelimeleri
    fakulte_map = {
        'Teknoloji FakÃ¼ltesi': ['teknoloji', 'bilgisayar', 'yazÄ±lÄ±m', 'programlama', 'mÃ¼hendislik'],
        'TÄ±p FakÃ¼ltesi': ['tÄ±p', 'hastane', 'klinik', 'cerrahi', 'anatomi', 'hekimlik'],
        'DiÅŸ HekimliÄŸi FakÃ¼ltesi': ['diÅŸ hekimliÄŸi', 'diÅŸ', 'dental'],
        'Veteriner FakÃ¼ltesi': ['veteriner', 'hayvan', 'veterinerlik'],
        'Hukuk FakÃ¼ltesi': ['hukuk', 'kanun', 'mahkeme', 'dava', 'avukat'],
        'GÃ¼zel Sanatlar FakÃ¼ltesi': ['gÃ¼zel sanatlar', 'resim', 'heykel', 'mÃ¼zik'],
        'SaÄŸlÄ±k Bilimleri FakÃ¼ltesi': ['saÄŸlÄ±k bilimleri', 'fizyoterapi', 'beslenme'],
        'HemÅŸirelik FakÃ¼ltesi': ['hemÅŸirelik', 'hemÅŸire'],
        'MÃ¼hendislik FakÃ¼ltesi': ['mÃ¼hendislik', 'inÅŸaat', 'makine', 'elektrik'],
        'Ziraat FakÃ¼ltesi': ['ziraat', 'tarÄ±m', 'Ã§iftÃ§ilik'],
        'Dilek SabancÄ± Devlet KonservatuarÄ±': ['konservatuvar', 'mÃ¼zik', 'sahne'],
    }

    for fakulte_name, keywords in fakulte_map.items():
        if any(kw in query_lower for kw in keywords):
            print(f"ğŸ¯ Otomatik tespit: {fakulte_name}")
            return fakulte_name

    return None


# ============================================================================
# YARDIMCI FONKSÄ°YONLAR
# ============================================================================

def normalize_text(text: str) -> str:
    """Metni normalize et"""
    text = text.lower()
    text = re.sub(r'[^\wÄŸÃ¼ÅŸÃ¶Ã§Ä±Ä°\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def extract_keywords(query: str) -> List[str]:
    """Anahtar kelimeleri Ã§Ä±kar"""
    stop_words = {
        'bir', 'bu', 've', 'ile', 'iÃ§in', 'mi', 'mÄ±', 'mu', 'mÃ¼',
        'da', 'de', 'ta', 'te', 'ben', 'sen', 'bana', 'sana',
        'ne', 'nedir', 'nasÄ±l', 'gibi', 'olan', 'olarak',
        'verir', 'misin', 'misiniz', 'var', 'yok', 'ÅŸey',
        'hakkÄ±nda', 'konusunda', 'ile', 'alakalÄ±', 'ilgili',
        'bilgi', 'verme', 'vermek', 'sÃ¶yle', 'anlat',
        'eder', 'olur', 'dÄ±r', 'dir', 'tir', 'tÄ±r'
    }

    words = normalize_text(query).split()
    keywords = [w for w in words if w not in stop_words and len(w) > 2]

    return keywords


def hybrid_search(query: str, fakulte_filter: Optional[str] = None, top_k: int = 5) -> List[Dict]:
    """
    ğŸ”¥ v7.0: GENEL METADATA FÄ°LTRE + FAKÃœLTE PARAMETRESÄ°
    """
    query_normalized = normalize_text(query)

    # "yandal" â†’ "yan dal"
    if 'yandal' in query_normalized:
        query_normalized = query_normalized.replace('yandal', 'yan dal')
        print(f"ğŸ”„ Query normalize: 'yandal' â†’ 'yan dal'")

    # "Ã§iftanadal" â†’ "Ã§ift anadal"
    if 'Ã§iftanadal' in query_normalized or 'Ã§iftanaadal' in query_normalized:
        query_normalized = query_normalized.replace('Ã§iftanadal', 'Ã§ift anadal')
        query_normalized = query_normalized.replace('Ã§iftanaadal', 'Ã§ift anadal')
        print(f"ğŸ”„ Query normalize: 'Ã§ift anadal'")

    # Query expansion
    query_expansions = {
        'cap': 'Ã§ift anadal Ã§ap',
        'Ã§ap': 'Ã§ift anadal Ã§ap',
        'gano': 'genel aÄŸÄ±rlÄ±klÄ± not ortalamasÄ±',
    }

    for short, expanded in query_expansions.items():
        if short in query_normalized:
            query_normalized = query_normalized.replace(short, expanded)
            print(f"ğŸ”„ Query geniÅŸletme: '{short}' â†’ '{expanded}'")

    keywords = extract_keywords(query_normalized)
    print(f"ğŸ” Anahtar kelimeler: {keywords}")

    # 1. BM25 Search
    bm25_scores = bm25.get_scores(keywords)

    # 2. ğŸ†• GENEL METADATA FÄ°LTRE
    try:
        where_filter = None

        # ğŸ†• 1. KullanÄ±cÄ± UI'dan fakÃ¼lte seÃ§tiyse
        if fakulte_filter:
            where_filter = {
                "$or": [
                    {"fakulte": {"$eq": fakulte_filter}},
                    {"belge_tipi": {"$eq": "university_general"}}
                ]
            }
            print(f"ğŸ¯ UI SeÃ§imi: {fakulte_filter}")

        # ğŸ†• 2. SeÃ§mediyse otomatik tespit et
        else:
            detected_fakulte = detect_fakulte(query)

            # LisansÃ¼stÃ¼ tespiti
            is_lisansustu = any(term in query_normalized for term in [
                "lisansÃ¼stÃ¼", "yÃ¼ksek lisans", "doktora", "master", "phd", "tezli", "tezsiz"
            ])

            # Pedagojik formasyon tespiti
            is_pedagojik = "pedagojik" in query_normalized or "formasyon" in query_normalized

            # FakÃ¼lte tespit edildiyse ve lisansÃ¼stÃ¼/pedagojik deÄŸilse filtrele
            if detected_fakulte and not is_lisansustu and not is_pedagojik:
                where_filter = {
                    "$or": [
                        {"fakulte": {"$eq": detected_fakulte}},
                        {"belge_tipi": {"$eq": "university_general"}}
                    ]
                }
                print(f"ğŸ¯ Otomatik tespit: {detected_fakulte}")

        # Semantic search
        if where_filter:
            semantic_results = db.similarity_search_with_score(
                query_normalized,
                k=top_k * 3,
                filter=where_filter
            )
        else:
            semantic_results = db.similarity_search_with_score(query_normalized, k=top_k * 3)

    except Exception as e:
        print(f"âš ï¸  Semantic search hatasÄ±: {e}")
        semantic_results = []

    # 3. SonuÃ§larÄ± birleÅŸtir
    candidates = {}

    for idx, score in enumerate(bm25_scores):
        if score > 0:
            doc_id = f"doc_{idx}"
            candidates[doc_id] = {
                'doc': all_documents[idx],
                'bm25_score': float(score),
                'semantic_score': 0.0,
                'content': all_documents[idx]['icerik']
            }

    for doc, distance in semantic_results:
        content = doc.page_content
        for idx, d in enumerate(all_documents):
            if d['icerik'] == content:
                doc_id = f"doc_{idx}"
                semantic_score = 1 - distance

                if doc_id in candidates:
                    candidates[doc_id]['semantic_score'] = semantic_score
                else:
                    candidates[doc_id] = {
                        'doc': d,
                        'bm25_score': 0.0,
                        'semantic_score': semantic_score,
                        'content': content
                    }
                break

    # 4. KEYWORD BOOST + N-GRAM
    domain_keywords = {
        'teknoloji': ['teknoloji fakÃ¼ltesi', 'bilgisayar mÃ¼hendisliÄŸi', 'yazÄ±lÄ±m'],
        'bilgisayar': ['bilgisayar mÃ¼hendisliÄŸi', 'teknoloji', 'yazÄ±lÄ±m'],
        'yazÄ±lÄ±m': ['bilgisayar', 'programlama', 'kod'],
        'proje': ['mezuniyet projesi', 'bitirme projesi', 'tasarÄ±m projesi'],
        'mezuniyet': ['mezuniyet projesi', 'bitirme projesi'],
        'staj': ['staj', 'uygulama', 'iÅŸ yeri', 'zorunlu staj'],
        'gano': ['genel aÄŸÄ±rlÄ±klÄ± not ortalamasÄ±', 'ortalama'],
        'Ã§ift': ['Ã§ift anadal', 'Ã§ap'],
        'yan': ['yan dal', 'yandal'],
        'sss': ['sÄ±kÃ§a sorulan', 'soru', 'cevap'],
    }

    # N-gram (Ã§ift kelime)
    query_words = query_normalized.split()
    query_bigrams = []
    for i in range(len(query_words) - 1):
        bigram = f"{query_words[i]} {query_words[i + 1]}"
        query_bigrams.append(bigram)

    if query_bigrams:
        print(f"ğŸ” Bigrams: {query_bigrams}")

    for doc_id, result in candidates.items():
        content_lower = result['content'].lower()
        belge_lower = result['doc']['belge'].lower()

        keyword_boost = 0
        match_count = 0

        # Ã‡ift kelime boost
        for bigram in query_bigrams:
            if bigram in content_lower:
                keyword_boost += 5.0
                match_count += 1.5
                print(f"    ğŸ¯ Bigram eÅŸleÅŸme: '{bigram}'")

            if bigram in belge_lower:
                keyword_boost += 15.0
                match_count += 3.0
                print(f"    ğŸ’ Belge adÄ±nda bigram: '{bigram}'")

        # Tek kelime boost
        for kw in keywords:
            if kw in content_lower:
                match_count += 1
                keyword_boost += 2.0

            if kw in belge_lower:
                keyword_boost += 10.0
                match_count += 2
                print(f"    ğŸ¯ Belge adÄ± eÅŸleÅŸme: '{kw}'")

            # Domain keywords
            if kw in domain_keywords:
                related_words = domain_keywords[kw]
                for rel_word in related_words:
                    if rel_word in content_lower:
                        keyword_boost += 1.5
                        match_count += 0.5
                        break
                    if rel_word in belge_lower:
                        keyword_boost += 3.0
                        match_count += 1.0
                        break

        if keywords:
            match_ratio = match_count / len(keywords)
            keyword_boost *= match_ratio

        result['keyword_boost'] = keyword_boost
        result['match_count'] = match_count

    # 5. ğŸ†• GENEL METADATA BOOST
    print("\nğŸ·ï¸  Metadata boost uygulanÄ±yor...")
    query_lower = query.lower()

    for doc_id, result in candidates.items():
        doc = result['doc']
        belge_lower = doc['belge'].lower()

        belge_tipi = doc.get('belge_tipi', 'other')
        oncelik = doc.get('oncelik', 5)
        fakulte = doc.get('fakulte')

        # FakÃ¼lte kontrolÃ¼
        fakulte_in_query = False
        if fakulte:
            fakulte_lower = fakulte.lower()
            fakulte_in_query = fakulte_lower in query_lower

        # Pedagojik formasyon ceza
        is_pedagojik_belge = "pedagojik" in belge_lower or "formasyon" in belge_lower
        is_pedagojik_query = "pedagojik" in query_lower or "formasyon" in query_lower

        if is_pedagojik_belge and not is_pedagojik_query:
            result['keyword_boost'] -= 8.0
            print(f"  â›” Pedagojik ceza: {doc['belge'][:50]}")

        # ğŸ†• GENEL BOOST SÄ°STEMÄ°
        if belge_tipi == 'university_general':
            result['keyword_boost'] += 8.0
            print(f"  âœ¨ Genel yÃ¶netmelik: {doc['belge'][:50]}")

        elif belge_tipi == 'faculty_specific':
            if fakulte and fakulte_in_query:
                result['keyword_boost'] += 12.0  # Ä°stenilen fakÃ¼lte
                print(f"  ğŸ”¥ FakÃ¼lte eÅŸleÅŸme: {fakulte}")
            elif fakulte:
                # BaÅŸka fakÃ¼lte adÄ± soruda geÃ§iyorsa ceza
                other_fakulte_keywords = ['tÄ±p', 'hukuk', 'mÃ¼hendislik', 'teknoloji',
                                          'ziraat', 'veteriner', 'gÃ¼zel sanatlar']
                if any(kw in query_lower for kw in other_fakulte_keywords):
                    result['keyword_boost'] -= 4.0
                    print(f"  âš ï¸  YanlÄ±ÅŸ fakÃ¼lte ceza: {fakulte}")

        elif belge_tipi == 'program_specific':
            result['keyword_boost'] += (oncelik * 0.5)

        elif belge_tipi == 'low_priority':
            result['keyword_boost'] -= 2.0

        result['priority_raw'] = oncelik

    # 6. Final Skor
    print("\nğŸ¯ Final skor hesaplanÄ±yor...")
    for doc_id in candidates:
        c = candidates[doc_id]

        if c['match_count'] < 0.5 and c['semantic_score'] < 0.7:
            c['final_score'] = 0.0
            continue

        priority_raw = c.get('priority_raw', 5)
        priority_normalized = (priority_raw - 3) / 7.0
        priority_normalized = max(0, min(1, priority_normalized))

        c['final_score'] = (
                c['bm25_score'] * 0.35 +
                c['semantic_score'] * 0.20 +
                c['keyword_boost'] * 0.25 +
                priority_normalized * 0.20
        )

        if priority_raw >= 9:
            print(f"  ğŸ”¥ YÃ¼ksek Ã¶ncelik: {c['doc']['belge'][:50]}")

        if c['final_score'] < 2.0:
            c['final_score'] = 0.0

    # 7. SÄ±rala
    sorted_results = sorted(
        [c for c in candidates.values() if c['final_score'] > 0],
        key=lambda x: x['final_score'],
        reverse=True
    )

    # DEBUG
    if len(sorted_results) > 0:
        print("\n" + "=" * 80)
        print("ğŸ” DEBUG: Top 10 SonuÃ§")
        print("=" * 80)
        for i, r in enumerate(sorted_results[:10], 1):
            doc = r['doc']
            print(f"\n{i}. {doc['belge'][:60]}")
            print(f"   Madde: {doc['madde_no']}")
            print(f"   ğŸ“Š BM25: {r['bm25_score']:.2f} | Semantic: {r['semantic_score']:.2f}")
            print(f"   ğŸ·ï¸  Boost: {r['keyword_boost']:.2f} | Priority: {r.get('priority_raw', 5)}")
            print(f"   â­ FINAL: {r['final_score']:.2f}")
            print(f"   ğŸ›ï¸  FakÃ¼lte: {doc.get('fakulte', 'yok')}")
        print("=" * 80 + "\n")

    return sorted_results[:top_k]


def create_llm_answer(results: List[Dict], query: str, temperature: float = 0.3) -> str:
    """Gemini ile cevap Ã¼ret"""
    if not results:
        return "ÃœzgÃ¼nÃ¼m, bu konuda mevzuatlarda ilgili bilgi bulamadÄ±m. LÃ¼tfen sorunuzu farklÄ± kelimelerle ifade etmeyi deneyin."

    if not gemini_available:
        return create_fallback_answer(results, query)

    context = ""
    for i, result in enumerate(results[:3], 1):
        doc = result['doc']
        context += f"\n\n--- KAYNAK {i} ---\n"
        context += f"Belge: {doc['belge']}\n"
        context += f"Madde No: {doc['madde_no']}\n"
        if doc.get('fikra_no'):
            context += f"FÄ±kra No: {doc['fikra_no']}\n"
        context += f"Ä°Ã§erik:\n{doc['icerik']}\n"

    prompt = f"""Sen SelÃ§uk Ãœniversitesi'nin mevzuat konusunda uzman bir asistansÄ±n.

GÃ–REV: AÅŸaÄŸÄ±daki mevzuat maddelerini kullanarak kullanÄ±cÄ±nÄ±n sorusunu cevapla.

KULLANICI SORUSU:
{query}

Ä°LGÄ°LÄ° MEVZUAT MADDELERÄ°:
{context}

Ã–NEMLÄ° KURALLAR:
1. **SADECE** verilen mevzuat maddelerindeki bilgileri kullan
2. EÄŸer soruyla tam alakalÄ± bilgi yoksa, "Verilen mevzuat maddelerinde bu konuda aÃ§Ä±k bir bilgi bulamadÄ±m" de
3. CevabÄ±nÄ± TÃ¼rkÃ§e, net, anlaÅŸÄ±lÄ±r ve yapÄ±landÄ±rÄ±lmÄ±ÅŸ ÅŸekilde ver
4. Madde ve fÄ±kra numaralarÄ±nÄ± belirt
5. Gereksiz tekrar yapma, direkt cevapla

CEVAP:"""

    try:
        response = client.models.generate_content(
            model=MODEL_NAME,
            contents=prompt,
            config={
                'temperature': temperature,
                'max_output_tokens': 4096,
            }
        )

        answer = response.text
        answer += "\n\n---\n\n### ğŸ“š Kaynaklar\n\n"
        for i, result in enumerate(results[:3], 1):
            doc = result['doc']
            kaynak = f"**{i}.** {doc['belge']} - Madde {doc['madde_no']}"
            if doc.get('fikra_no'):
                kaynak += f", FÄ±kra {doc['fikra_no']}"
            answer += kaynak + "\n"

        return answer

    except Exception as e:
        print(f"âŒ LLM hatasÄ±: {e}")
        return create_fallback_answer(results, query)


def create_fallback_answer(results: List[Dict], query: str) -> str:
    """Fallback"""
    if not results:
        return "ÃœzgÃ¼nÃ¼m, bu konuda bilgi bulamadÄ±m."

    answer = f"### Ä°lgili Mevzuat Maddeleri\n\n"

    for i, result in enumerate(results[:3], 1):
        doc = result['doc']
        content = doc['icerik'][:500].strip()
        if len(doc['icerik']) > 500:
            content += "..."

        answer += f"#### {i}. {doc['belge']} - Madde {doc['madde_no']}\n\n"
        answer += f"{content}\n\n---\n\n"

    answer += "âš ï¸  **Not:** LLM aktif deÄŸil.\n"
    return answer


# ============================================================================
# API ENDPOINTS
# ============================================================================

@app.get("/")
def read_root():
    return {
        "message": "SelÃ§uk Ãœniversitesi Mevzuat Chatbot API",
        "status": "online",
        "toplam_madde": len(all_documents),
        "llm_aktif": gemini_available,
        "model": "BERTurk + Gemini + Fakulte Filtre v7.0"
    }


@app.post("/chat", response_model=ChatResponse)
def chat(q: Question):
    if not q.question.strip():
        raise HTTPException(status_code=400, detail="Soru boÅŸ olamaz!")

    print(f"\n{'=' * 70}")
    print(f"ğŸ” Soru: {q.question}")
    if q.fakulte_filter:
        print(f"ğŸ›ï¸  FakÃ¼lte filtresi: {q.fakulte_filter}")
    print(f"ğŸŒ¡ï¸  Temperature: {q.temperature}")

    # ğŸ†• FakÃ¼lte filtresi ile arama
    results = hybrid_search(q.question, fakulte_filter=q.fakulte_filter, top_k=q.top_k)

    if not results:
        print("âŒ SonuÃ§ yok")
        return ChatResponse(
            question=q.question,
            answer="ÃœzgÃ¼nÃ¼m, bu konuda mevzuatlarda ilgili bilgi bulamadÄ±m.",
            sources=[],
            session_id=q.session_id
        )

    print(f"\nğŸ“š {len(results)} sonuÃ§ bulundu:")
    for i, r in enumerate(results[:5], 1):
        doc = r['doc']
        print(f"   {i}. {doc['belge'][:60]} - M{doc['madde_no']} (skor: {r['final_score']:.2f})")

    sources = []
    for r in results:
        doc = r['doc']
        kaynak_metni = f"{doc['belge']} - Madde {doc['madde_no']}"
        if doc.get('fikra_no'):
            kaynak_metni += f", FÄ±kra {doc['fikra_no']}"

        sources.append({
            "belge": doc['belge'],
            "madde_no": doc['madde_no'],
            "fikra_no": doc.get('fikra_no'),
            "kaynak_metni": kaynak_metni,
            "icerik": doc['icerik'][:400] + "..." if len(doc['icerik']) > 400 else doc['icerik'],
            "score": round(r['final_score'], 2),
            "priority": r.get('priority_raw', 5)
        })

    answer = create_llm_answer(results, q.question, q.temperature)

    print(f"âœ… Cevap hazÄ±r ({len(answer)} karakter)")
    print(f"{'=' * 70}\n")

    return ChatResponse(
        question=q.question,
        answer=answer,
        sources=sources,
        session_id=q.session_id
    )


@app.get("/health")
def health_check():
    return {
        "status": "healthy",
        "documents": len(all_documents),
        "llm_available": gemini_available
    }


@app.delete("/session/{session_id}")
def delete_session(session_id: str):
    print(f"ğŸ—‘ï¸  Session silindi: {session_id}")
    return {"message": "Session silindi", "session_id": session_id}


if __name__ == "__main__":
    import uvicorn

    print("\n" + "=" * 70)
    print("ğŸš€ Mevzuat Chatbot API v7.0 (Fakulte Filtre)")
    print("=" * 70)
    print(f"ğŸ“š Toplam Madde: {len(all_documents)}")
    print(f"ğŸ¤– Embedding: BERTurk")
    print(f"ğŸ§  LLM: {'Gemini âœ…' if gemini_available else 'Yok âŒ'}")
    print(f"ğŸ” Search: Hybrid + N-gram + Fakulte Filtre")
    print("=" * 70)
    print("\nğŸ“¡ API: http://localhost:8000")
    print("ğŸ“– Docs: http://localhost:8000/docs\n")

    uvicorn.run(app, host="0.0.0.0", port=8000)
